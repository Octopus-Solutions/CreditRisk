{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Credit Risk Prediction\"\n",
    "author: \"Fang Zhou, Data Scientist, Microsoft\"\n",
    "date: \"`r Sys.Date()`\"\n",
    "output: html_document\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "include": "FALSE,",
     "purl": "FALSE"
    }
   },
   "outputs": [],
   "source": [
    "knitr::opts_chunk$set(echo = TRUE,\n",
    "                      fig.width = 8,\n",
    "                      fig.height = 5,\n",
    "                      fig.align='center',\n",
    "                      dev = \"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Introduction\n",
    "\n",
    "Credit Risk Scoring is a classic but increasingly important operation in banking as banks are becoming far more risk careful when lending for mortgages or commercial purposes, in an industry known for fierce competition and the global financial crisis. With an accurate credit risk scoring model a bank is able to predict the likelihood of default on a transaction. This will in turn help evaluate the potential risk posed by lending money to consumers and to mitigate losses due to bad debt, as well as determine who qualifies for a loan, at what interest rate, and what credit limits, and even determine which customers are likely to bring in the most revenue through a variety of products.\n",
    "\n",
    "Many banks nowadays are driving innovation to enhance risk management. For example, a largest bank in one of the Asian countries by market capitalization is exploring opportunities to segment a millions of active credit card\n",
    "customer population to improve risk scoring to then identify opportunities to offer increased limits. Using advanced analytics for credit risk scoring involves traditional scorecard building and modelling, and extends to machine learning and ensemble, but will also pursue an innovation on customer oriented aggregation of transactions, multi-dimensional customer segmentation and conceptual clustering to identify multiple segments across which to understand bank customers.\n",
    "\n",
    "In the data-driven credit risk predition model, normally two types of data are taken into consideration.\n",
    "\n",
    "1. **Transaction data** The transaction records covering account id, transaction date, transaction amount, merchant industry, ect. This transaction-level data could be dynamically aggregated and then provide transaction statistics and financial behavior information at account level.\n",
    "\n",
    "2. **Demographic and bank account information** This type of data show the characteristics of individual customer or account credit bureau, such as age, sex, income, and credit limit. They are static and never change or solely increment deterministically over time.\n",
    "\n",
    "## 2 Data Driven Credit Risk Prediction\n",
    "\n",
    "### 2.1 Setup \n",
    "\n",
    "We load the required R packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "error": "FALSE",
     "id": "",
     "message": "FALSE,",
     "warning": "FALSE,"
    }
   },
   "outputs": [],
   "source": [
    "## Setup\n",
    "\n",
    "# Load the required packages into the R session.\n",
    "\n",
    "library(rattle)       # The normVarNames().\n",
    "library(readr)        # Fast read csv file.\n",
    "library(plyr)         # Wrangling.\n",
    "library(dplyr)        # Wrangling: tbl_df(), group_by(), print(), glimpse().\n",
    "library(tidyr)        # Wrangling: gather().\n",
    "library(magrittr)     # Pipe operator %>% %<>% %T>% equals().\n",
    "library(lubridate)    # Convert character to datetime: mdy_hms().\n",
    "library(ggplot2)      # Visualization.\n",
    "library(stringi)      # String concat operator %s+%.\n",
    "library(glmnet)       # Build lasso logistic regression with glmnet().\n",
    "library(e1071)        # Build support vector machine model with svm().\n",
    "library(randomForest) # Build random forest model with randomForest().\n",
    "library(Matrix)       # Construct a Matrix of a class that inherits from Matrix.\n",
    "library(xgboost)      # Build extreme gradiant boosting with xgboost(). \n",
    "#library(Ckmeans.1d.dp)# Required for xgb.plot.importance().\n",
    "library(caret)        # Delete near zero variables with nearZeroVar() and train model with train().\n",
    "library(caretEnsemble)# Build model ensemble. \n",
    "library(ROCR)         # Draw ROC Curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Data Ingestion\n",
    "\n",
    "We use the transaction and demographic datasets simulated from public data and real data from a financial institute to conduct this R accelerator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "error": "FALSE",
     "id": "",
     "message": "FALSE,",
     "warning": "FALSE,"
    }
   },
   "outputs": [],
   "source": [
    "## Data Ingestion\n",
    "\n",
    "# Identify the source location of the dataset.\n",
    "\n",
    "#DATA <- \"../../Data/\"\n",
    "#txn_fname <- file.path(DATA, \"Raw/transactionSimu.csv\")\n",
    "#demog_fname <- file.path(DATA, \"Raw/demographicSimu.csv\")\n",
    "\n",
    "wd <- getwd()\n",
    "\n",
    "dpath <- \"Data\"\n",
    "txn_fname <- file.path(wd, dpath, \"transactionSimu_v3.csv\")\n",
    "demog_fname <- file.path(wd, dpath, \"demographicSimu_v3.csv\")\n",
    "\n",
    "# Ingest the dataset.\n",
    "\n",
    "txn <- read_csv(file=txn_fname)\n",
    "demog <- read_csv(file=demog_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Data Preparation\n",
    "\n",
    "Before analyzing, we normalize variable names and variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "error": "FALSE",
     "id": "",
     "message": "FALSE,",
     "warning": "FALSE,"
    }
   },
   "outputs": [],
   "source": [
    "# Normalize variable names.\n",
    "\n",
    "names(txn) %<>% normVarNames() %>% print()\n",
    "names(demog) %<>% normVarNames() %>% print()\n",
    "\n",
    "# Normalize variables.\n",
    "\n",
    "txn %>% \n",
    "  sapply(is.character) %>%\n",
    "  which() %>%\n",
    "  names() ->\n",
    "txnc\n",
    "\n",
    "demog %>% \n",
    "  sapply(is.character) %>%\n",
    "  which() %>%\n",
    "  names() ->\n",
    "demc\n",
    "\n",
    "txn[txnc] %<>% sapply(normVarNames)\n",
    "demog[demc] %<>% sapply(normVarNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Data Exploration and Preprocessing\n",
    "\n",
    "#### 2.4.1 Data Exploration \n",
    "\n",
    "##### 2.4.1.1 Transaction Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "error": "FALSE",
     "id": "",
     "message": "FALSE,",
     "warning": "FALSE,"
    }
   },
   "outputs": [],
   "source": [
    "## Explore transaction data.\n",
    "\n",
    "# Review datasets.\n",
    "\n",
    "dim(txn)\n",
    "\n",
    "# A glimpse into the data.\n",
    "\n",
    "glimpse(txn)\n",
    "\n",
    "# Review observations.\n",
    "\n",
    "head(txn) %>% print.data.frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transaction data set contains 198,576 records and 11 variables. The columns of \"transaction_id\" and \"account_id\" are identifiers at transaction level and account level, respectively. Each account has more than one transaction records occuring at different date or time. These transaction records show information about transaction amount, transaction type, location, merchant and so on. \n",
    "\n",
    "Initial exploratory analysis can be performed to get a general understanding of the dataset. For example,\n",
    "\n",
    "1. The transaction frequency of each account varies. The following plot shows the top 10 accounts with the highest transaction frequency, i.e., number of purchase in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "message": "FALSE"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the top 10 accounts with the highest transaction frequency.\n",
    "\n",
    "txn %>%\n",
    "  filter(transaction_type == \"p\") %>%\n",
    "  select(account_id) %>%\n",
    "  group_by(account_id) %>%\n",
    "  count() %>%\n",
    "  arrange(desc(n)) %>%\n",
    "  head(n=10) %>%\n",
    "  ggplot(aes(x=account_id, y=n)) +\n",
    "  geom_bar(stat=\"identity\", position=\"dodge\", fill=\"lightblue\") +\n",
    "  labs(x=\"Account ID\",\n",
    "       y=\"Transaction Frequency (Purchase)\",\n",
    "       title=\"Top 10 Accounts with the Higest Transaction Frequency\",\n",
    "       caption=\"Source: \" %s+% \"Transaction\") +\n",
    "  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The total transaction amount made by different types of credit cards vary across merchant industry, and this can be plotted as shown below. The total transaction amount in bank industry is much higher than that in any other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "message": "FALSE"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the distribution of total purchase across merchant industry. \n",
    "\n",
    "txn %>%\n",
    "  filter(transaction_type == \"p\") %>%\n",
    "  select(merchant_industry, card_type, transaction_amount_usd) %>%\n",
    "  group_by(merchant_industry) %>% \n",
    "  mutate(amount_per_industry=log(sum(transaction_amount_usd))) %>%\n",
    "  ggplot(aes(x=merchant_industry, y=amount_per_industry, fill=card_type)) +\n",
    "  geom_bar(stat=\"identity\") +\n",
    "  labs(x=\"Merchant Industry\",\n",
    "       y=\"Log of Transaction Amount (USD)\",\n",
    "       fill=\"Card Type\",\n",
    "       title=\"Total Transaction Amount Across Merchant Industry\\n by Card Type\",\n",
    "       caption=\"Source: \" %s+% \"Transaction\") +\n",
    "  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.4.1.2 Demographic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "error": "FALSE",
     "id": "",
     "message": "FALSE,",
     "warning": "FALSE,"
    }
   },
   "outputs": [],
   "source": [
    "## Explore demographic data.\n",
    "\n",
    "# Review datasets.\n",
    "\n",
    "dim(demog)\n",
    "\n",
    "# A glimpse into the data.\n",
    "\n",
    "glimpse(demog)\n",
    "\n",
    "# Review observations.\n",
    "\n",
    "head(demog) %>% print.data.frame() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The demographic data set contains 184,254 records and 9 variables. It has a common key, \"account_id\", with the transaction data. The column of \"bad_flag\" is the label of customers or accounts (assuming one customer one account), representing their default status. The other variables show information about the characteristic of customers (e.g., age, education, income) and bank account information (e.g., credit limit). \n",
    "\n",
    "Visual understanding of the dataset can be achieved by the following plots.\n",
    "\n",
    "1. The proportion of customers for status of \"default\" and \"non-default\" within different education levels (or any other possible factor) may vary. People hold degrees of \"middle school\" and \"high school\", are among the top 2 groups that exhibit highest default rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "message": "FALSE"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the impact of education on default status.\n",
    "\n",
    "demog %>%\n",
    " ggplot(aes(x=factor(education), fill=factor(bad_flag))) +\n",
    " geom_bar(width=0.5, position=\"fill\") +\n",
    " coord_flip() +\n",
    " scale_fill_discrete(guide=guide_legend(title=\"Bad Flag\")) +\n",
    " labs(x=\"Education Level\",\n",
    "      y=\"Proportion\",\n",
    "      fill=\"Bad Flag\",\n",
    "      title=\"Proportion of Customers for Default Status\\n within Each Education Level\",\n",
    "      caption=\"Source: \" %s+% \"Demographic\") +\n",
    " theme(axis.text.x = element_text(angle = 45, hjust = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Credit limit (or income), sex and marital status may affect the default status of the customers with different education levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "message": "FALSE"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the impact of credit limit/income on default status across education level.\n",
    "\n",
    "demog %>%\n",
    "  filter(sex == \"male\" & marital_status == \"single\") %>%\n",
    "  ggplot(aes(x=factor(education), y=credit_limit, color=factor(bad_flag))) +\n",
    "  geom_boxplot() +\n",
    "  scale_fill_discrete(guide=guide_legend(title=\"Bad Flag\")) +\n",
    "  labs(x=\"Education Level\",\n",
    "       y=\"Credit Limit\",\n",
    "       color=\"Bad Flag\",\n",
    "       title=\"Boxplot of Credit Limit Across Education Level\\n by Default Status\",\n",
    "       caption=\"Source: \" %s+% \"Demographic\") +\n",
    " theme(axis.text.x = element_text(angle = 45, hjust = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.2 Data Aggregation\n",
    "\n",
    "Since the analytical target is to predict credit risk at account level, then our first task is to aggregate the transaction data up to an account view with a single row for each account, aggregating each transaction, whether it is spends, purchase, cash withdraw, or fee and then joining. Here we only provide a simple example for the limited sample dataset, where all transaction type is equal to purchase. The time window for aggregation is 6 months, from 2013-04-01 UTC to 2013-09-30 UTC, for this sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "message": "FALSE,",
     "warning": "FALSE"
    }
   },
   "outputs": [],
   "source": [
    "## Data Aggregation\n",
    "\n",
    "# Define some functions to simply the calculation.\n",
    "\n",
    "# Calculate maximum interval of a time series.\n",
    "\n",
    "maxInterval <- function(ts) {\n",
    "  if (length(na.omit(ts)) <= 1) {\n",
    "    interval <- 0\n",
    "  } else {\n",
    "    interval <- as.numeric(max(difftime(c(ts[-1], NA), ts, units=\"day\"), na.rm=TRUE))\n",
    "  }\n",
    "  return(interval)\n",
    "}\n",
    "\n",
    "# Calculate minimum interval of a time series.\n",
    "\n",
    "minInterval <- function(ts) {\n",
    "  if (length(na.omit(ts)) <= 1) {\n",
    "    interval <- 0\n",
    "  } else {\n",
    "    interval <- as.numeric(min(difftime(c(ts[-1], NA), ts, units=\"day\"), na.rm=TRUE))\n",
    "  }\n",
    "  return(interval)\n",
    "}\n",
    "\n",
    "# Calculate average interval of a time series.\n",
    "\n",
    "avgInterval <- function(ts) {\n",
    "  if (length(na.omit(ts)) <= 1) {\n",
    "    interval <- 0\n",
    "  } else {\n",
    "    interval <- as.numeric(mean(difftime(c(ts[-1], NA), ts, units=\"day\"), na.rm=TRUE))\n",
    "  }\n",
    "  return(interval)\n",
    "}\n",
    "\n",
    "# Calculate interval of time between enddate and the most recent transaction date.\n",
    "\n",
    "recentInterval <- function(startdate, enddate, ts) {\n",
    "  if (length(na.omit(ts)) == 0) {\n",
    "   interval <- as.numeric(difftime(enddate, startdate, units=\"day\"))\n",
    "  } else {\n",
    "    interval <- as.numeric(difftime(enddate, max(ts, na.rm=TRUE), units=\"day\"))\n",
    "  }\n",
    "  return(interval)\n",
    "}\n",
    "\n",
    "# Aggregate transaction data per account level.\n",
    "\n",
    "txn %>%\n",
    "    filter(transaction_type == \"p\") %>%\n",
    "    group_by(account_id) %>%\n",
    "    arrange(account_id, transaction_date) %>%\n",
    "    summarise(amount_6      = sum(transaction_amount_usd),\n",
    "              pur_6         = n(),\n",
    "              bank_6        = ifelse(pur_6 == 0, 0, sum(merchant_industry == \"bank\")/pur_6),\n",
    "              entmnt_6      = ifelse(pur_6 == 0, 0, sum(merchant_industry == \"entmnt\")/pur_6),\n",
    "              jewellery_6   = ifelse(pur_6 == 0, 0, sum(merchant_industry == \"jewellery\")/pur_6),\n",
    "              medical_6     = ifelse(pur_6 == 0, 0, sum(merchant_industry == \"medical\")/pur_6),\n",
    "              others_6      = ifelse(pur_6 == 0, 0, sum(merchant_industry == \"others\")/pur_6),\n",
    "              petrol_6      = ifelse(pur_6 == 0, 0, sum(merchant_industry == \"petrol\")/pur_6),\n",
    "              restaurant_6  = ifelse(pur_6 == 0, 0, sum(merchant_industry == \"restaurant\")/pur_6),\n",
    "              supermarket_6 = ifelse(pur_6 == 0, 0, sum(merchant_industry == \"supermkt\")/pur_6),\n",
    "              telecom_6     = ifelse(pur_6 == 0, 0, sum(merchant_industry == \"telecom\")/pur_6),\n",
    "              travel_6      = ifelse(pur_6 == 0, 0, sum(merchant_industry == \"travel\")/pur_6),\n",
    "              utility_6     = ifelse(pur_6 == 0, 0, sum(merchant_industry == \"utility\")/pur_6),\n",
    "              avg_pur_amt_6 = ifelse(pur_6 == 0, 0, sum(transaction_amount_usd)/pur_6),\n",
    "              max_pur_amt_6 = max(transaction_amount_usd),\n",
    "              min_pur_amt_6 = min(transaction_amount_usd),\n",
    "              avg_interval_pur_6 = avgInterval(transaction_date),\n",
    "              max_interval_pur_6 = maxInterval(transaction_date),\n",
    "              min_interval_pur_6 = minInterval(transaction_date),\n",
    "              last_pur_time_6    = recentInterval(min(transaction_date), max(transaction_date), transaction_date)\n",
    "              ) ->\n",
    "rollup\n",
    "\n",
    "# Review aggregated data.\n",
    "\n",
    "glimpse(rollup)\n",
    "  \n",
    "# Check the number of infinite values for each variable.\n",
    "\n",
    "unlist(lapply(rollup, function(x) sum(is.infinite(x))))\n",
    "\n",
    "# Check the number of missing values for each variable.\n",
    "\n",
    "unlist(lapply(rollup, function(x) sum(is.na(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.3 Data Merging\n",
    "\n",
    "Let's merge the aggregated transaction data with demographic data by their common key, account id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "error": "FALSE",
     "id": "",
     "message": "FALSE,",
     "warning": "FALSE,"
    }
   },
   "outputs": [],
   "source": [
    "## Merge aggregated transaction data with demographic data.\n",
    "\n",
    "merged <- merge(rollup, demog, by=c(\"account_id\"))\n",
    "\n",
    "glimpse(merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.4 Data Cleansing\n",
    "\n",
    "Before getting into feature engineering, data cleansing work, including variable cleansing and missing value cleansing, are essential and will finally help improve the prediction performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "error": "FALSE",
     "id": "",
     "message": "FALSE,",
     "warning": "FALSE,"
    }
   },
   "outputs": [],
   "source": [
    "## Variable roles.\n",
    "\n",
    "# Target variable\n",
    "\n",
    "target <- \"bad_flag\"\n",
    "\n",
    "# Note any identifier.\n",
    "\n",
    "id <- c(\"account_id\") %T>% print() \n",
    "\n",
    "# Note the available variables.\n",
    "\n",
    "vars <- names(merged) %T>% print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "error": "FALSE",
     "id": "",
     "message": "FALSE,",
     "warning": "FALSE,"
    }
   },
   "outputs": [],
   "source": [
    "## Variable cleansing \n",
    "\n",
    "# Initialise ignored variables: identifiers\n",
    "\n",
    "ignore <- id %T>% print()\n",
    "\n",
    "# Identify variables with only missing values.\n",
    "\n",
    "merged[vars] %>%\n",
    "  sapply(function(x) x %>% is.na %>% sum) %>%\n",
    "  equals(nrow(merged)) %>%\n",
    "  which() %>%\n",
    "  names() %T>%\n",
    "  print() ->\n",
    "missing\n",
    "\n",
    "# Add them if any to the variables to be ignored for modelling.\n",
    "\n",
    "ignore <- union(ignore, missing) %T>% print()\n",
    "\n",
    "# Identify a threshold above which proportion missing is fatal.\n",
    "\n",
    "missing.threshold <- 0.5\n",
    "\n",
    "# Identify variables that are mostly missing.\n",
    "\n",
    "merged[vars] %>%\n",
    "  sapply(function(x) x %>% is.na() %>% sum()) %>%\n",
    "  '>'(missing.threshold*nrow(merged)) %>%\n",
    "  which() %>%\n",
    "  names() %T>%\n",
    "  print() ->\n",
    "mostly\n",
    "\n",
    "# Add it into variable to be ignore.\n",
    "\n",
    "ignore <- union(ignore, mostly) %T>% print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "error": "FALSE",
     "id": "",
     "message": "FALSE,",
     "warning": "FALSE,"
    }
   },
   "outputs": [],
   "source": [
    "## Missing value cleansing.\n",
    "\n",
    "# Count the number of missing values.\n",
    "\n",
    "merged[vars] %>%  is.na() %>% sum()\n",
    "\n",
    "# Identify variables with some missing values but not too much.\n",
    "\n",
    "merged[vars] %>%\n",
    "  sapply(function(x) x %>% is.na %>% sum) %>%\n",
    "  '>' (0) %>%\n",
    "  which() %>%\n",
    "  names() %T>%\n",
    "  print() ->\n",
    "some\n",
    "\n",
    "merged[some] %>% lapply(class) %>% print()\n",
    "\n",
    "# Identify categorical variables with some missing values.\n",
    "\n",
    "merged[some] %>%\n",
    "  sapply(is.character) %>%\n",
    "  which(useNames=TRUE) %>%\n",
    "  names() %T>% \n",
    "  print() ->\n",
    "some.catc\n",
    "\n",
    "# Identify numeric variables with some missing values.\n",
    "\n",
    "merged[some] %>%\n",
    "  sapply(is.numeric) %>%\n",
    "  which(useNames=TRUE) %>%\n",
    "  names() %T>% \n",
    "  print() ->\n",
    "some.numc\n",
    "\n",
    "# Compute the mode of each categorical variables.\n",
    "\n",
    "merged[some.catc] %>% \n",
    "  lapply(table) %>% \n",
    "  print() ->\n",
    "counts.table\n",
    "\n",
    "some.catc.mode <- unlist(lapply(counts.table,\n",
    "                                FUN=function(x){as.character(names(x[which.max(x)]))}),\n",
    "                         use.names=FALSE)\n",
    "\n",
    "# Compute the mean of each numeric variables.\n",
    "\n",
    "merged[some.numc] %>%\n",
    "  lapply(mean, na.rm=TRUE) %>%\n",
    "  unlist(use.names=F) %>%\n",
    "  print() ->\n",
    "some.numc.mean\n",
    "\n",
    "# Define a function to replace missing values with mean or mode. \n",
    "\n",
    "meanModeReplace <- function(data, numc, numc.mean, catc, catc.mode) {\n",
    "  \n",
    "  data <- data.frame(data)\n",
    "    \n",
    " # Replace numeric variables with the mean. \n",
    " \n",
    " if(length(numc) > 0) \n",
    "   for(i in 1:length(numc)) {\n",
    "    row.na <- which(is.na(data[, numc[i]]) == TRUE) \n",
    "    data[row.na, numc[i]] <- numc.mean[i]\n",
    "  }\n",
    " \n",
    " # Replace character variables with the mode. \n",
    " \n",
    " if(length(catc) > 0)\n",
    "   for(i in 1:length(catc)) {\n",
    "    row.na <- which(is.na(data[, catc[i]]) == TRUE) \n",
    "    data[row.na, catc[i]] <- catc.mode[i]\n",
    "  }\n",
    " \n",
    " return(data)\n",
    " \n",
    "}  \n",
    "  \n",
    "# Impute missing values.\n",
    "\n",
    "merged[vars] %<>% \n",
    "  meanModeReplace(numc=some.numc,\n",
    "                  numc.mean=some.numc.mean, \n",
    "                  catc=some.catc,\n",
    "                  catc.mode=some.catc.mode) \n",
    "\n",
    "# Confirm that no missing values remain.\n",
    "\n",
    "merged[vars] %>%  is.na() %>% sum()\n",
    "\n",
    "# Identify variables that have a single value.\n",
    "\n",
    "merged[vars] %>%\n",
    "  sapply(function(x) all(x == x[1L])) %>%\n",
    "  which() %>%\n",
    "  names() %T>%\n",
    "  print() ->\n",
    "constants\n",
    "\n",
    "# Add them to the variable to be ignored.\n",
    "\n",
    "ignore <- union(ignore, constants) %>% print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "error": "FALSE,",
     "id": "",
     "message": "FALSE,",
     "warning": "FALSE"
    }
   },
   "outputs": [],
   "source": [
    "# Clean correlated variables.\n",
    "\n",
    "# Note which variables are numeric.\n",
    "\n",
    "vars %>%\n",
    "  setdiff(ignore) %>%\n",
    "  '['(merged, .) %>%\n",
    "  sapply(is.numeric) %>% \n",
    "  which() %>%\n",
    "  names() %T>%\n",
    "  print() ->\n",
    "numc\n",
    "\n",
    "# For the numeric variables generate a table of correlations\n",
    "\n",
    "merged[numc] %>%\n",
    "  cor(use=\"complete.obs\") %>%\n",
    "  ifelse(upper.tri(., diag=TRUE), NA, .) %>% \n",
    "  abs %>% \n",
    "  data.frame %>%\n",
    "  tbl_df %>%\n",
    "  set_colnames(numc) %>%\n",
    "  mutate(var1=numc) %>% \n",
    "  gather(var2, cor, -var1) %>% \n",
    "  na.omit %>%\n",
    "  arrange(-abs(cor)) %T>%\n",
    "  print() ->\n",
    "mc\n",
    "\n",
    "# Add variables with collinearity to the variable to be ignored.\n",
    "\n",
    "ignore <- union(ignore, NULL) %>% print()\n",
    "\n",
    "# Check the number of variables currently.\n",
    "\n",
    "length(vars)\n",
    "\n",
    "# Remove the variables to ignore.\n",
    "\n",
    "vars <- setdiff(vars, ignore) %T>% print()\n",
    "\n",
    "# Confirm they are now ignored.\n",
    "\n",
    "length(vars)\n",
    "\n",
    "# Convert all categorical variables into factor.\n",
    "\n",
    "merged %>% \n",
    "  sapply(is.character) %>%\n",
    "  which(useNames=TRUE) %>%\n",
    "  names() ->\n",
    "catc\n",
    "  \n",
    "merged[catc] %<>% lapply(factor)\n",
    "\n",
    "# Review the processed dataset.\n",
    "\n",
    "processed <- merged[c(id, vars)]\n",
    "\n",
    "glimpse(processed)\n",
    "\n",
    "# Export the merged and cleansed data.\n",
    "\n",
    "write.csv(processed, file=file.path(wd, dpath, \"processedSimu.csv\"), row.names=FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Scorecard Method\n",
    "\n",
    "The classic and still widely used (and useful) approach for evaluating credit worthiness and risk is based on the building of \"scorecards\". There are several aspects of the particular modeling workflow for producing a scorecard, and for using it effectively.\n",
    "\n",
    "1. **Discretizing Predictors** A scorecard needs to make it easy for it's user to determine the individual components contributing to the overall score and credit decision, and to achieve that, it is useful to divide the values of each continuous or categorical predictor variable into a relatively small number of categories so that an applicant can be quickly scored. There are a number of methods to re-code variable values into a small number of classes, for example, **binning analytics** by using quantiles or smbinning.\n",
    "\n",
    "2. **Logistic Regression Modeling** Given a binary variable indicating good or bad account, Logistic Regression models are well suited for subsequent predictive modeling. It provides a logit-transformed linear relationship between the prediction probability and the predictors. When the number of predictors are relatively large or there exists multi-collinearity, **Lasso Logistic Regression**, provided by the function glmnet(), outperforms all the other. This algorithm is extremely fast, and can exploit sparsity in the model input matrix. \n",
    "\n",
    "3. **Machine Learning Algorithms** If the accuracy of the prediction of risk is the most important consideration of a scorecard building project, then machine learning models, such as **Gradient Boosting** and **Random Forest**, or their **ensembles**, provide better performance than simple logistic regression models. Since most algorithms are general approximators capable of representing any relationship between predictors and outcomes, and are also relatively robust to outliers, it is not necessary to perform many of the feature engineering steps such as binning, etc. \n",
    "\n",
    "#### 2.5.1 Feature Engineering\n",
    "\n",
    "Firstly, we do feature engineering by discretizing predictors, i.e., binning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "error": "FALSE",
     "id": "",
     "message": "FALSE,",
     "warning": "FALSE,"
    }
   },
   "outputs": [],
   "source": [
    "# Note the variable names in the processed dataset.\n",
    "\n",
    "vars <- names(processed)\n",
    "\n",
    "# Identify numeric variables used for binning.\n",
    "\n",
    "processed %>%\n",
    "  sapply(is.numeric) %>%\n",
    "  which(useNames=TRUE) %>%\n",
    "  names() %T>% \n",
    "  print() ->\n",
    "vars_fe\n",
    "\n",
    "# Compute quantiles with an even split of 10% for numeric variables. \n",
    "\n",
    "quantiles_names <- setdiff(vars_fe, c(\"income\"))\n",
    "\n",
    "q <- lapply(1:length(quantiles_names), \n",
    "            function(i, data) {\n",
    "              quantile(x=processed[[quantiles_names[i]]], \n",
    "                       data=processed, \n",
    "                       probs=seq(0, 1, 1/10), \n",
    "                       na.rm=TRUE)\n",
    "            })\n",
    "\n",
    "names(q) <- c(quantiles_names)\n",
    "\n",
    "# Define uneven bins for the rest of the numeric variables. \n",
    "  \n",
    "uneven_bins_names <- c(\"income\")\n",
    "  \n",
    "b <- list(income = c(0, 100000, 2000000, 400000))\n",
    "\n",
    "# Define function to bucketize numeric variables.\n",
    "\n",
    "bucketize <- function(data, q, quantiles_names, b, uneven_bins_names) {\n",
    "    \n",
    "  data <- data.frame(data)\n",
    "    \n",
    "  # Bucketize based on quantiles.\n",
    "  \n",
    "  for(name in  quantiles_names) {\n",
    "      name2 <- paste(name, \"_bucket\", sep = \"\")\n",
    "      data[, name2] <- as.character(length(q[[name]]))\n",
    "      for(i in seq(1, (length(q[[name]]) - 1))) {\n",
    "        rows <- which(data[, name] < q[[name]][[i + 1]] & data[, name] >= q[[name]][[i]])\n",
    "        data[rows, name2] <- as.character(i)\n",
    "      }\n",
    "  }\n",
    "  \n",
    "  # Bucketize based on manually defined bins. \n",
    "  \n",
    "  for(name in  uneven_bins_names) {\n",
    "      name2 <- paste(name, \"_bucket\", sep = \"\")\n",
    "      data[, name2] <- as.character(length(b[[name]]))\n",
    "      for(i in seq(1, (length(b[[name]]) - 1))) {\n",
    "        rows <- which(data[, name] < b[[name]][i + 1] & data[, name] >= b[[name]][i])\n",
    "        data[rows, name2] <- as.character(i)\n",
    "      }\n",
    "  }\n",
    "  \n",
    "  return(data) \n",
    "  \n",
    "}  \n",
    "\n",
    "# Create bucketized variables.\n",
    "\n",
    "binned <- bucketize(data=processed, \n",
    "                    q=q, \n",
    "                    quantiles_names=quantiles_names, \n",
    "                    b=b, \n",
    "                    uneven_bins_names=uneven_bins_names)\n",
    "\n",
    "# Save the new column names for future use. \n",
    "  \n",
    "bucketized_old <- c(quantiles_names, uneven_bins_names)\n",
    "  \n",
    "for(i in 1:length(bucketized_old)) {\n",
    "    nameB <- paste(bucketized_old[i], \"_bucket\", sep =\"\")\n",
    "    vars[[length(vars) + 1]] <- nameB\n",
    "}\n",
    "\n",
    "# Drop variables used for feature engineering.\n",
    "\n",
    "vars <- setdiff(vars, vars_fe)\n",
    "\n",
    "binned <- binned[vars]\n",
    "\n",
    "# Convert all categorical variables into factor.\n",
    "\n",
    "binned %>% \n",
    "  sapply(is.character) %>%\n",
    "  which(useNames=TRUE) %>%\n",
    "  names() ->\n",
    "catc\n",
    "  \n",
    "binned[catc] %<>% lapply(factor)\n",
    "\n",
    "glimpse(binned)\n",
    "\n",
    "# Export the result.\n",
    "\n",
    "write.csv(binned, file=file.path(wd, dpath, \"binnedSimu.csv\"), row.names=FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.2 Model Building and Evaluation\n",
    "\n",
    "In this section, we get started to build and evaluate the model. This section will introduce how a model is constructed for credit risk prediction. Normally credit risk prediction is categorized as a classification problem. Depending on the variety of labels, the problem can be either binary-classification or multi-classification.\n",
    "\n",
    "The case in this example is a binary-classification problem. The label for prediction is default status, i.e., \"bad_flag\", which has two levels, 'yes' and 'no'. \n",
    "\n",
    "It is possible that not all the variables are correlated with the label, therefore, feature selection is usually performed to eliminate the less relevant ones.\n",
    "\n",
    "As the data set is a blend of numeric and discrete variables (or discrete variables only), correlation analysis is not applicable. One alternative is to train a model, which can do automatic feature selection in the process of model building. Such model includes lasso logistic regression, extreme gradiant boosting, random forest, and so on. \n",
    "\n",
    "##### 2.5.2.1. Lasso Logistic Regression Model\n",
    "\n",
    "First of all, the lasso logistic regression model, is applied on data after binning, considering it is very sensitive to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "error": "FALSE",
     "id": "",
     "message": "FALSE,",
     "warning": "FALSE,"
    }
   },
   "outputs": [],
   "source": [
    "# Introduce a generic variables.\n",
    "\n",
    "data <- binned\n",
    "\n",
    "vars <- setdiff(names(data), c(target, id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is split into training and testing datasets for model creation and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "error": "FALSE",
     "id": "",
     "message": "FALSE,",
     "warning": "FALSE,"
    }
   },
   "outputs": [],
   "source": [
    "# Split Data\n",
    "\n",
    "set.seed(42)\n",
    "\n",
    "data <- data[order(runif(nrow(data))), ]\n",
    "\n",
    "train <- sample(nrow(data), 0.70 * nrow(data))\n",
    "test <- setdiff(seq_len(nrow(data)), train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train a lasso logistic regression model as the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "error": "FALSE",
     "id": "",
     "message": "FALSE,",
     "warning": "FALSE,"
    }
   },
   "outputs": [],
   "source": [
    "# Matrix preparation for glmnet model\n",
    "\n",
    "form <- as.formula(paste(target, paste(vars, collapse=\"+\"), sep=\"~\"))\n",
    "form\n",
    "\n",
    "xfactors <- model.matrix(form, data)[, -1]\n",
    "x <- as.matrix(data.frame(xfactors))\n",
    "\n",
    "bad_flag <- data$bad_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "error": "FALSE",
     "id": "",
     "message": "FALSE,",
     "warning": "FALSE,"
    }
   },
   "outputs": [],
   "source": [
    "## Train lasso logistic regression model.\n",
    "\n",
    "# Note alpha=1 for lasso only and can blend with ridge penalty down to alpha=0 ridge only.\n",
    "\n",
    "model_glmnet <- glmnet(x[train, ], y=bad_flag[train], alpha=1, family=\"binomial\")\n",
    "\n",
    "# Plot variable coefficients vs. shrinkage parameter lambda.\n",
    "\n",
    "# plot(model_glmnet, xvar=\"lambda\")\n",
    "\n",
    "# Obtain optimal lambda using cross validation.\n",
    "\n",
    "model_glmnet_cv <- cv.glmnet(x[train, ], y=bad_flag[train], alpha=1, family=\"binomial\")\n",
    "\n",
    "# plot(model_glmnet_cv)\n",
    "\n",
    "best_lambda <- model_glmnet_cv$lambda.min %T>% print()\n",
    "\n",
    "# Obtain the coefficient of each variable in the optimal fitted model, which shows variable importance as well.\n",
    "\n",
    "coef(model_glmnet_cv, s=best_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once a (lasso logistic regression) model has been built based on a training data set, next the validity of the model needs to be assessed in a testing data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "error": "FALSE",
     "id": "",
     "message": "FALSE,",
     "warning": "FALSE,"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate model performance on the validation dataset. \n",
    "\n",
    "# Obtain the probabilities of credit default for the fitted optimal glmnet() model.\n",
    "\n",
    "probs <- predict(model_glmnet_cv, newx=x[test, ], type=\"response\", s=best_lambda)\n",
    "\n",
    "# Create a prediction object.\n",
    "\n",
    "pred <- ROCR::prediction(predictions=probs, \n",
    "                         labels=data[test, ]$bad_flag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictive power of the model can be assessed via various ways, for example, useful graphs (e.g., Kolmogorov Smirnov chart, roc curve, lift chart) and confusion matrix. The results are as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "error": "FALSE",
     "id": "",
     "message": "FALSE,",
     "warning": "FALSE,"
    }
   },
   "outputs": [],
   "source": [
    "# Draw Kolmogorov Smirnov chart. \n",
    "\n",
    "score1 <- pred@predictions[[1]][pred@labels[[1]]==\"no\"]\n",
    "score2 <- pred@predictions[[1]][pred@labels[[1]]==\"yes\"]\n",
    "group <- c(rep(\"score1\", length(score1)), rep(\"score2\", length(score2)))\n",
    "dat <- data.frame(KSD=c(score1, score2), group=group)\n",
    "\n",
    "# Create ECDF of data\n",
    "\n",
    "cdf1 <- ecdf(score1) \n",
    "cdf2 <- ecdf(score2) \n",
    "\n",
    "# Find min and max statistics to draw line between points of greatest distance\n",
    "\n",
    "minMax <- seq(min(score1, score2), max(score1, score2), length.out=length(score1))\n",
    "\n",
    "# Compute KS\n",
    "\n",
    "ks <- max(abs(cdf1(minMax) - cdf2(minMax))) %>% print()\n",
    "\n",
    "# Find the predicted probability where the cumulative distributions have the biggest difference. \n",
    "x0 <- minMax[which(abs(cdf1(minMax) - cdf2(minMax)) == ks)] \n",
    "y0 <- cdf1(x0)\n",
    "y1 <- cdf2(x0)\n",
    "\n",
    "dat %>%\n",
    "ggplot(aes(x=KSD, group=group, color = group)) +\n",
    "  stat_ecdf(size=1) +\n",
    "    geom_segment(aes(x=x0[1], y=y0[1], xend=x0[1], yend=y1[1]),\n",
    "                 linetype = \"dashed\", color = \"red\") +\n",
    "    geom_point(aes(x=x0[1], y=y0[1]), color=\"red\", size=5) +\n",
    "    geom_point(aes(x=x0[1], y=y1[1]), color=\"red\", size=5) +\n",
    "    annotate(\"text\", x=0.3, y=0.00, hjust=1, vjust=0, size=5,\n",
    "              label=paste(\"KS =\", round(ks, 3))) +\n",
    "    labs(x=\"Score\",\n",
    "         y=\"ECDF\",\n",
    "         title=\"KS Plot for Lasso Logistic Regression Model\",\n",
    "         caption=\"Source: \" %s+% \"glmnet\") +\n",
    "    theme(axis.text.x=element_text(hjust=1))\n",
    "\n",
    "# Draw ROC curve\n",
    "\n",
    "perf <- ROCR::performance(pred, \"tpr\", \"fpr\")\n",
    "auc <- ROCR::performance(pred, \"auc\")@y.values[[1]] %>% print()\n",
    "pd <- data.frame(fpr=unlist(perf@x.values), tpr=unlist(perf@y.values))\n",
    "\n",
    "pd %>%\n",
    " ggplot(aes(x=fpr, y=tpr)) +\n",
    "   geom_line(colour=\"red\") +\n",
    "   geom_line(data=data.frame(), aes(x=c(0,1), y=c(0,1)), colour=\"grey\") +\n",
    "   annotate(\"text\", x=0.50, y=0.00, hjust=0, vjust=0, size=5,\n",
    "             label=paste(\"AUC =\", round(auc, 3))) +\n",
    "   labs(x=\"False Positive Rate\",\n",
    "        y=\"True Positive Rate\",\n",
    "        title=\"ROC Curve for Lasso Logistic Regression Model\",\n",
    "        caption=\"Source: \" %s+% \"glmnet\") +\n",
    "   theme(axis.text.x = element_text(hjust=1))\n",
    "\n",
    "# Calculate the confusion matrix\n",
    " \n",
    "df_pred <- data.frame(cbind(data[test, ], probs))\n",
    "colnames(df_pred)[colnames(df_pred)==\"X1\"] <- \"scored_probs\"\n",
    "df_pred %<>% mutate(scored_label=ifelse(scored_probs > 0.5, \"yes\", \"no\"))\n",
    "\n",
    "confusionMatrix(data=df_pred$scored_label, \n",
    "                reference=data[test, ]$bad_flag, \n",
    "                positive=\"yes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.5.2.2 Extreme Gradiant Boosting\n",
    "\n",
    "As mentioned earlier, it is not necessary to conduct binning analytics, when applying some machine learning algorithms, like extreme gradiant boosting, etc. Here, we build the model on the processed data directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "error": "FALSE",
     "id": "",
     "message": "FALSE,",
     "warning": "FALSE,"
    }
   },
   "outputs": [],
   "source": [
    "# Introduce a generic variable.\n",
    "\n",
    "data <- processed\n",
    "\n",
    "vars <- setdiff(names(data), c(target, id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "error": "FALSE",
     "id": "",
     "message": "FALSE,",
     "warning": "FALSE,"
    }
   },
   "outputs": [],
   "source": [
    "# Train model: xgboost\n",
    "\n",
    "ntrain <- as.matrix(sapply(data[train, vars], as.numeric))\n",
    "\n",
    "dtrain <- list()\n",
    "dtrain$data <- Matrix(ntrain, sparse=TRUE)\n",
    "dtrain$label <- as.numeric(data[train, target]) - 1\n",
    "\n",
    "dtrain %>% str()\n",
    "\n",
    "model_xgb <- xgboost(data=dtrain$data, \n",
    "                       label=dtrain$label,\n",
    "                       booster=\"gbtree\",\n",
    "                       max_depth=6, \n",
    "                       eta=0.3, \n",
    "                       nthread=2, \n",
    "                       nround=150, \n",
    "                       verbose=0,\n",
    "                       eval_metrics=list(\"error\"),\n",
    "                       objective=\"binary:logistic\")\n",
    "\n",
    "# Calculate feature importance\n",
    "\n",
    "imp <- xgb.importance(feature_names=dtrain$data@Dimnames[[2]], \n",
    "                      model=model_xgb)\n",
    "\n",
    "print(imp)\n",
    "\n",
    "# Visualize feature importance\n",
    "\n",
    "# xgb.plot.importance(imp)\n",
    "\n",
    "# Display tree number 1\n",
    "\n",
    "#xgb.plot.tree(feature_names=colnames(dtrain$data),\n",
    "#              model=model_xgb,\n",
    "#              n_first_tree=1)\n",
    "\n",
    "# Score model\n",
    "\n",
    "ntest <- as.matrix(sapply(data[test, vars], as.numeric))\n",
    "\n",
    "dtest <- list()\n",
    "dtest$data <- Matrix(ntest, sparse=TRUE)\n",
    "dtest$label <- as.numeric(data[test, target]) - 1\n",
    "\n",
    "dtest %>% str()\n",
    "\n",
    "probs <- predict(model_xgb, dtest$data)\n",
    "\n",
    "# Create a prediction object.\n",
    "\n",
    "pred <- ROCR::prediction(predictions=probs, \n",
    "                         labels=data[test, ]$bad_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "error": "FALSE",
     "id": "",
     "message": "FALSE,",
     "warning": "FALSE,"
    }
   },
   "outputs": [],
   "source": [
    "# Draw Kolmogorov Smirnov chart. \n",
    "\n",
    "score1 <- pred@predictions[[1]][pred@labels[[1]]==\"no\"]\n",
    "score2 <- pred@predictions[[1]][pred@labels[[1]]==\"yes\"]\n",
    "group <- c(rep(\"score1\", length(score1)), rep(\"score2\", length(score2)))\n",
    "dat <- data.frame(KSD=c(score1,score2), group=group)\n",
    "\n",
    "# Create ECDF of data\n",
    "\n",
    "cdf1 <- ecdf(score1) \n",
    "cdf2 <- ecdf(score2) \n",
    "\n",
    "# Find min and max statistics to draw line between points of greatest distance\n",
    "\n",
    "minMax <- seq(min(score1, score2), max(score1, score2), length.out=length(score1))\n",
    "\n",
    "# Compute KS\n",
    "\n",
    "ks <- max(abs(cdf1(minMax) - cdf2(minMax))) %>% print()\n",
    "\n",
    "# Find the predicted probability where the cumulative distributions have the biggest difference. \n",
    "\n",
    "x0 <- minMax[which(abs(cdf1(minMax) - cdf2(minMax)) == ks)] \n",
    "y0 <- cdf1(x0)\n",
    "y1 <- cdf2(x0)\n",
    "\n",
    "dat %>%\n",
    "ggplot(aes(x=KSD, group=group, color=group)) +\n",
    "  stat_ecdf(size=1) +\n",
    "    geom_segment(aes(x=x0[1], y=y0[1], xend=x0[1], yend=y1[1]),\n",
    "                 linetype=\"dashed\", color=\"red\") +\n",
    "    geom_point(aes(x=x0[1], y=y0[1]), color=\"red\", size=5) +\n",
    "    geom_point(aes(x=x0[1], y=y1[1]), color=\"red\", size=5) +\n",
    "    annotate(\"text\", x=0.3, y=0.00, hjust=0.1, vjust=0, size=5,\n",
    "            label=paste(\"KS =\", round(ks, 3))) +\n",
    "    labs(x=\"Score\",\n",
    "         y=\"ECDF\",\n",
    "         title=\"KS Plot for Extreme Gradiant Boost Model\",\n",
    "         caption=\"Source: \" %s+% \"xgboost\") +\n",
    "    theme(axis.text.x=element_text(hjust=1))\n",
    "\n",
    "# Draw ROC curve\n",
    "\n",
    "perf <- ROCR::performance(pred, \"tpr\", \"fpr\")\n",
    "auc <- ROCR::performance(pred, \"auc\")@y.values[[1]] %>% print()\n",
    "pd <- data.frame(fpr=unlist(perf@x.values), tpr=unlist(perf@y.values))\n",
    "\n",
    "pd %>%\n",
    " ggplot(aes(x=fpr, y=tpr)) +\n",
    "   geom_line(colour=\"red\") +\n",
    "   geom_line(data=data.frame(), aes(x=c(0,1), y=c(0,1)), colour=\"grey\") +\n",
    "   annotate(\"text\", x=0.50, y=0.00, hjust=0, vjust=0, size=5,\n",
    "             label=paste(\"AUC =\", round(auc, 3))) +\n",
    "   labs(x=\"False Positive Rate\",\n",
    "        y=\"True Positive Rate\",\n",
    "        title=\"ROC Curve for Extreme Gradiant Boosting Model\",\n",
    "        caption=\"Source: \" %s+% \"xgboost\") +\n",
    "   theme(axis.text.x=element_text(hjust=1))\n",
    "\n",
    "# Calculate the confusion matrix\n",
    " \n",
    "df_pred <- data.frame(cbind(data[test, ], scored_probs=probs))\n",
    "df_pred %<>% mutate(scored_label=ifelse(scored_probs > 0.5, \"yes\", \"no\"))\n",
    "\n",
    "confusionMatrix(data=df_pred$scored_label, \n",
    "                reference=data[test, ]$bad_flag, \n",
    "                positive=\"yes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.5.2.3 Parameter Tuning and Model Ensembles\n",
    "\n",
    "1. Model-specific feature selection.\n",
    "\n",
    "Here, we train an extreme gradiant boosting model and then rank the feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "error": "FALSE",
     "id": "",
     "message": "FALSE,",
     "warning": "FALSE,"
    }
   },
   "outputs": [],
   "source": [
    "# Set up the training control.\n",
    "\n",
    "control <- trainControl(method=\"repeatedcv\", number=5, repeats=3)\n",
    "\n",
    "# Train the model\n",
    "\n",
    "model <- caret::train(bad_flag ~ .,\n",
    "                      data[c(target, vars)],\n",
    "                      method=\"xgbTree\",\n",
    "                      preProcess=\"scale\",\n",
    "                      allowParallel=TRUE,\n",
    "                      trControl=control) \n",
    "\n",
    "# Estimate variable importance\n",
    "\n",
    "imp <- varImp(model, scale=FALSE)\n",
    "\n",
    "# Plot\n",
    "\n",
    "plot(imp)\n",
    "\n",
    "# Select the top-ranking variables.\n",
    "\n",
    "imp_list <- rownames(imp$importance)[order(imp$importance$Overall, decreasing=TRUE)]\n",
    "\n",
    "# Drop the low ranking variables.\n",
    "\n",
    "top_vars <- \n",
    "  imp_list[1:(ncol(data) - 6)] %>%\n",
    "  as.character() \n",
    "\n",
    "top_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Individual models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "error": "FALSE",
     "id": "",
     "message": "FALSE,",
     "warning": "FALSE,"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize training control.\n",
    "\n",
    "tc <- trainControl(method=\"boot\", \n",
    "                   number=5, \n",
    "                   repeats=3, \n",
    "                   search=\"grid\",\n",
    "                   classProbs=TRUE,\n",
    "                   savePredictions=\"final\",\n",
    "                   summaryFunction=twoClassSummary,\n",
    "                   allowParallel=TRUE)\n",
    "\n",
    "# Extreme gradiant boosting model.\n",
    "\n",
    "time_xgb <- system.time(\n",
    "  model_xgb <- caret::train(bad_flag ~ .,\n",
    "                      data[train, c(target, vars)],\n",
    "                      method=\"xgbTree\",\n",
    "                      trainControl=tc)\n",
    ")\n",
    "\n",
    "# Random forest model\n",
    "\n",
    "time_rf <- system.time(\n",
    "  model_rf <- caret::train(bad_flag ~ .,\n",
    "                     data[train, c(target, vars)],\n",
    "                     method=\"rf\",\n",
    "                     trainControl=tc)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Ensemble of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "error": "FALSE",
     "id": "",
     "message": "FALSE,",
     "warning": "FALSE,"
    }
   },
   "outputs": [],
   "source": [
    "# Ensemble of models\n",
    "\n",
    "levels(data$bad_flag) <- make.names(levels(data$bad_flag))\n",
    "\n",
    "time_ensemble <- system.time(\n",
    "  model_list <- caretList(bad_flag ~ ., \n",
    "                          data=data[train, c(target, vars)],\n",
    "                          trControl=tc,\n",
    "                          methodList=c(\"xgbTree\", \"rf\"))\n",
    ")\n",
    "\n",
    "xyplot(resamples(model_list))\n",
    "modelCor(resamples(model_list))\n",
    "\n",
    "# Ensemble of models\n",
    "\n",
    "model_ensemble <- caretEnsemble(\n",
    "  model_list, \n",
    "  metric=\"ROC\",\n",
    "  trControl=tc)\n",
    "\n",
    "# Stack of models. Use gbm for meta model.\n",
    "\n",
    "model_stack <- caretStack(\n",
    "  model_list,\n",
    "  metric=\"ROC\",\n",
    "  method=\"gbm\",\n",
    "  verbose=FALSE,\n",
    "  trControl=tc)\n",
    "\n",
    "# Variable importance of model ensembles.\n",
    "\n",
    "# varImp(model_ensemble)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation and performance comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "error": "FALSE",
     "id": "",
     "message": "FALSE,",
     "warning": "FALSE,"
    }
   },
   "outputs": [],
   "source": [
    "# Predict\n",
    "\n",
    "models <- list(model_xgb, model_rf, model_ensemble, model_stack)\n",
    "\n",
    "# Predict class\n",
    "\n",
    "predictions <-lapply(models, \n",
    "                     predict, \n",
    "                     newdata=data[test, c(target, vars)])\n",
    "\n",
    "levels(predictions[[4]]) <- c(\"no\", \"yes\")\n",
    "\n",
    "# Confusion matrix evaluation results.\n",
    "\n",
    "cm_metrics <-lapply(predictions,\n",
    "                    confusionMatrix, \n",
    "                    reference=data[test, target],\n",
    "                    positive=\"yes\")\n",
    "\n",
    "# Accuracy\n",
    "\n",
    "acc_metrics <- \n",
    "  lapply(cm_metrics, `[[`, \"overall\") %>%\n",
    "  lapply(`[`, 1) %>%\n",
    "  unlist() %>%\n",
    "  as.vector()\n",
    "\n",
    "# Recall\n",
    "\n",
    "rec_metrics <- \n",
    "  lapply(cm_metrics, `[[`, \"byClass\") %>%\n",
    "  lapply(`[`, 1) %>%\n",
    "  unlist() %>%\n",
    "  as.vector()\n",
    "  \n",
    "# Precision\n",
    "\n",
    "pre_metrics <- \n",
    "  lapply(cm_metrics, `[[`, \"byClass\") %>%\n",
    "  lapply(`[`, 3) %>%\n",
    "  unlist() %>%\n",
    "  as.vector()\n",
    "\n",
    "# Predict class probability\n",
    "\n",
    "probs12 <- lapply(models[c(1, 2)],\n",
    "                predict,\n",
    "                newdata=data[test, c(target, vars)],\n",
    "                type=\"prob\") %>%\n",
    "         lapply('[[', 2)\n",
    "\n",
    "probs34 <- lapply(models[c(3, 4)],\n",
    "                predict,\n",
    "                newdata=data[test, c(target, vars)],\n",
    "                type=\"prob\")\n",
    "\n",
    "probs <- c(probs12, probs34)\n",
    "\n",
    "# Create prediction object\n",
    "\n",
    "preds <- lapply(probs, \n",
    "                ROCR::prediction,\n",
    "                labels=data[test, target])\n",
    "\n",
    "# Auc\n",
    "\n",
    "auc_metrics <- lapply(preds, \n",
    "                      ROCR::performance,\n",
    "                      \"auc\") %>%\n",
    "               lapply(slot, \"y.values\") %>%\n",
    "               lapply('[[', 1) %>%\n",
    "               unlist()\n",
    "\n",
    "algo_list <- c(\"Xgboost\", \"Random Forest\", \"Ensemble\", \"Stacking\")\n",
    "time_consumption <- c(time_xgb[3], time_rf[3], time_ensemble[3], time_ensemble[3])\n",
    "\n",
    "df_comp <- \n",
    "  data.frame(Models=algo_list, \n",
    "             Accuracy=acc_metrics, \n",
    "             Recall=rec_metrics, \n",
    "             Precision=pre_metrics,\n",
    "             AUC=auc_metrics,\n",
    "             Time=time_consumption) %T>%\n",
    "             {head(.) %>% print()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Conclusion\n",
    "\n",
    "This document introduces a data-driven advanced analytics for credit risk prediction. Techniques of exploratory analytics, data aggregation and cleansing, feature engineering, but more importantly, model building and evaluation are demonstrated on the simulated datasets. This walk-through may help banks to enhance risk managment and then identify opportunities to adjust credit limits and eventually reduce losses and increase revenue in lending. The later version of this document will pursue an innovative hotspot method on credit risk in future."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 1
}
